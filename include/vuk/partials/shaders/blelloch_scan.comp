#version 450
#pragma shader_stage(compute)

#extension GL_KHR_shader_subgroup_basic : require

#define SIZE 256
layout (local_size_x = SIZE) in;

layout (std430, binding = 0) buffer coherent readonly BufferIn {
	uint[] g_idata;
};

layout (std430, binding = 1) buffer coherent BufferOut {
	uint[] g_odata;
};

layout (std430, binding = 2) buffer coherent Temp {
    uint out_wg_count;
    uint out_unused[2];
    uint out_count;
	uint[] g_temp;
};

layout (std430, binding = 4) buffer BufferCount {
    readonly uint wg_count;
    uint tmp_wg_counter;
    uint unused;
    readonly uint count;
};

// https://developer.nvidia.com/gpugems/gpugems3/part-vi-gpu-computing/chapter-39-parallel-prefix-sum-scan-cuda
const uint array_size = 2*SIZE;
shared uint temp[array_size];

uint linear_index(uvec3 id){
	uvec3 mp = gl_NumWorkGroups * gl_WorkGroupSize;
	return id.z * mp.y * mp.x + id.y * mp.x + id.x * 1;
}

void main() {
    // needs: tmp_wg_counter == 1
    uint gid = linear_index(gl_GlobalInvocationID);
    uint thid = gl_LocalInvocationIndex;
    int offset = 1;
    uint n = array_size;
    temp[2*thid] = g_idata[2*gid].x; // load input into shared memory
    temp[2*thid+1] = g_idata[2*gid+1].x;

    for (uint d = n>>1; d > 0; d >>= 1) { // build sum in place up the tree
        barrier();
        if (thid < d) {
            uint ai = offset*(2*thid+1)-1;
            uint bi = offset*(2*thid+2)-1;
            temp[bi] += temp[ai];
        }
        offset *= 2;
    } 

    if (thid == 0) {
        g_temp[gl_WorkGroupID.x] = temp[n - 1];
        //g_temp[gl_WorkGroupID.x + array_size] = 0;
        temp[n - 1] = 0;  // clear the last element 
    }
 	
    for (int d = 1; d < n; d *= 2){ // traverse down tree & build scan 
        offset >>= 1;      
        barrier();
        if (thid < d) {
            uint ai = offset*(2*thid+1)-1;
            uint bi = offset*(2*thid+2)-1; 
            uint t = temp[ai]; 
            temp[ai] = temp[bi];
            temp[bi] += t;
        } 
    }  

    g_odata[2*gid].x = temp[2*thid]; // write results to device memory      
    g_odata[2*gid+1].x = temp[2*thid+1];

    if(gl_LocalInvocationIndex == 0){
        temp[0] = 0;
        if(atomicAdd(tmp_wg_counter, 1) != (gl_NumWorkGroups.x)){
            temp[0] = 1;
        }
    }
    barrier();
    if(temp[0] == 1){
        return;
    }
    atomicExchange(tmp_wg_counter, 1); // reset tmp counter for future dispatches
    memoryBarrier();
    if(count > 512 * 512) { // threshold for 3 level
        if(gl_LocalInvocationIndex == 0){
            out_wg_count = (count + 512 * 512 - 1) / (512 * 512);
            out_unused[0] = 1;
            out_unused[1] = 1;
            out_count = (count + 512 - 1) / 512;
        }
        return;
    }
    // surviving WG: reduction over temp (needs loop!)
    n = array_size;//count / array_size;
    gid = gl_LocalInvocationIndex;

    offset = 1;
    temp[2*thid] = g_temp[2*gid].x; // load input into shared memory
    temp[2*thid+1] = g_temp[2*gid+1].x;

    for (uint d = n>>1; d > 0; d >>= 1) { // build sum in place up the tree
        barrier();
        if (thid < d) {
            uint ai = offset*(2*thid+1)-1;
            uint bi = offset*(2*thid+2)-1;
            temp[bi] += temp[ai];
        }
        offset *= 2;
    } 

    if (thid == 0) {
        temp[n - 1] = 0;  // clear the last element 
    }
 	
    for (int d = 1; d < n; d *= 2){ // traverse down tree & build scan 
        offset >>= 1;
        barrier();
        if (thid < d) {
            uint ai = offset*(2*thid+1)-1;
            uint bi = offset*(2*thid+2)-1; 
            uint t = temp[ai]; 
            temp[ai] = temp[bi];
            temp[bi] += t;
        }
    }
    barrier();

    g_temp[2*gid + array_size].x = temp[2*thid]; // write results to device memory      
    g_temp[2*gid + array_size + 1].x = temp[2*thid+1];
}